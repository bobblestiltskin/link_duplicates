I have a number of backup disks - I buy bigger ones occasionally to back up
the smaller ones. I have a number of small computers with sdcard storage.
These are backed up to one of the smaller hard disks, which is backed up to a 
bigger one. I saw that I was duplicating files on the disk, so I wrote 
this to reduce the amount of duplication. Multiple copies of the same file on
the same disk is not safer than having one copy with hard links to the 
duplicates.

All of my current computers are running Debian GNU/Linux. I have not tested
the code elsewhere. It is written in Perl so ought to work elsewhere...

In brief, the algorithm ensures the following:

Only files which are the same size are considered as duplicates.
Only files of that size with multiple distinct inodes are considered.
Only files with duplicated md5sums are considered.
Only files with duplicated shasums are considered.

These tests are done in this order - which is the computationally cheapest
first. Any inodes which satisfy all of the conditions are considered
duplicated; all but one of the set is removed and hard links made from
the duplicate point to the first - they are identical. This will leave the
file system with the same contents, but occupying less disk space.

There is a script, makedata.sh, which generates a little test data. Feel free
to modify it to satisfy yourself that the algorithm behaves as expected.

	rm -rf /tmp/data
	mkdir -p /tmp/data/data1 /tmp/data/data2 /tmp/data/data3 /tmp/data/data4
	echo "abc123" > /tmp/data/data1/abc
	echo "abc123" > /tmp/data/data2/abc
	echo "abc123" > /tmp/data/data3/abc
	echo "abc123" > /tmp/data/data4/abc
	echo "def123" > /tmp/data/data2/def
	ln /tmp/data/data2/def /tmp/data/data1/def
	echo "def123" > /tmp/data/data4/def
	ln /tmp/data/data4/def /tmp/data/data3/def
	echo "ghi123" > /tmp/data/data1/ghi
	echo "ghi123" > /tmp/data/data4/ghi
	echo "jkl123" > /tmp/data/data3/jkl
	echo "xyz" > /tmp/data/data1/xyz

after running the script, we see ...
    
        $ sh makedata.sh
	$ find /tmp/data -type f -exec ls -l "{}" \; | sort
	-rw-r--r-- 1 bob bob 4 Jan 20 18:46 /tmp/data/data1/xyz
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data1/abc
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data1/ghi
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data2/abc
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data3/abc
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data3/jkl
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data4/abc
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data4/ghi
	-rw-r--r-- 2 bob bob 7 Jan 20 18:46 /tmp/data/data1/def
	-rw-r--r-- 2 bob bob 7 Jan 20 18:46 /tmp/data/data2/def
	-rw-r--r-- 2 bob bob 7 Jan 20 18:46 /tmp/data/data3/def
	-rw-r--r-- 2 bob bob 7 Jan 20 18:46 /tmp/data/data4/def
	$

The link count (nlink) is the first integer in the long listing.

So, if we run 

	$ perl link_duplicate.pl --verbose --dump 0 /tmp/data

This will find process all files with size > 10**0 == 1 in /tmp/data

The program runs find down the file tree for the directory chosen for 
#deduplication.
We are interested principally in the sizes of the files found (duplicate files 
MUST be the same size) and secondly with the inode number of the files found. 
We then store any file names found in a list at this inode.

	*** Finding location of files
	*** MINSIZE is 1
	wanted: file - /tmp/data/data2/abc, size - 7, inode - 82215
	wanted: file - /tmp/data/data2/def, size - 7, inode - 82216
	wanted: file - /tmp/data/data3/abc, size - 7, inode - 90467
	wanted: file - /tmp/data/data3/def, size - 7, inode - 213213
	wanted: file - /tmp/data/data3/jkl, size - 7, inode - 91252
	wanted: file - /tmp/data/data4/abc, size - 7, inode - 213212
	wanted: file - /tmp/data/data4/def, size - 7, inode - 213213
	wanted: file - /tmp/data/data4/ghi, size - 7, inode - 213214
	wanted: file - /tmp/data/data1/abc, size - 7, inode - 66628
	wanted: file - /tmp/data/data1/def, size - 7, inode - 82216
	wanted: file - /tmp/data/data1/ghi, size - 7, inode - 66630
	wanted: file - /tmp/data/data1/xyz, size - 4, inode - 66631
	$VAR1 = {
	          '7' => {
	                   '213212' => [
	                                 '/tmp/data/data4/abc'
	                               ],
	                   '82215' => [
	                                '/tmp/data/data2/abc'
	                              ],
	                   '66628' => [
	                                '/tmp/data/data1/abc'
	                              ],
	                   '91252' => [
	                                '/tmp/data/data3/jkl'
	                              ],
	                   '213213' => [
	                                 '/tmp/data/data3/def',
	                                 '/tmp/data/data4/def'
	                               ],
	                   '213214' => [
	                                 '/tmp/data/data4/ghi'
	                               ],
	                   '82216' => [
	                                '/tmp/data/data2/def',
	                                '/tmp/data/data1/def'
	                              ],
	                   '66630' => [
	                                '/tmp/data/data1/ghi'
	                              ],
	                   '90467' => [
	                                '/tmp/data/data3/abc'
	                              ]
	                 },
	          '4' => {
	                   '66631' => [
	                                '/tmp/data/data1/xyz'
	                              ]
	                 }
	        };

Here we see the results of the initial pass over the data. The two 
sizes are the primary keys of the data structure; the inodes the
secondary keys, and the associated files are listed. Our shell 
script links the def files explicitly, this detail can be seen in
the files listed for the inodes.

	*** Calculating md5sum of files
	md5sum_data: file - /tmp/data/data4/abc, checksum - 2c6c8ab6ba8b9c98a1939450eb4089ed
	md5sum_data: file - /tmp/data/data2/abc, checksum - 2c6c8ab6ba8b9c98a1939450eb4089ed
	md5sum_data: file - /tmp/data/data1/abc, checksum - 2c6c8ab6ba8b9c98a1939450eb4089ed
	md5sum_data: file - /tmp/data/data3/jkl, checksum - 072fb97d7ff234142ed0bea4bce29d0e
	md5sum_data: file - /tmp/data/data3/def, checksum - d93ef486ced011f786a650654d837f8c
	md5sum_data: file - /tmp/data/data4/ghi, checksum - 5efe41b654b978fd0ab27d8b26061cf0
	md5sum_data: file - /tmp/data/data2/def, checksum - d93ef486ced011f786a650654d837f8c
	md5sum_data: file - /tmp/data/data1/ghi, checksum - 5efe41b654b978fd0ab27d8b26061cf0
	md5sum_data: file - /tmp/data/data3/abc, checksum - 2c6c8ab6ba8b9c98a1939450eb4089ed

We then compute the md5sum of the distinct inodes - as can be seen above only
two of the def files - have have the checksums computed. The file of size 4 is
discarded from further consideration, since there is no other of that size.

	shasum_data: file - /tmp/data/data4/ghi, checksum - 77676b6a1b009237d7665bd234d10c066bc7fc3e5655cbd8707736f525e457227650a91b50984c9bf0f098f71a311b701bb67601f630d8f7507843a94a726b9a
	shasum_data: file - /tmp/data/data1/ghi, checksum - 77676b6a1b009237d7665bd234d10c066bc7fc3e5655cbd8707736f525e457227650a91b50984c9bf0f098f71a311b701bb67601f630d8f7507843a94a726b9a
	shasum_data: file - /tmp/data/data2/abc, checksum - 9760a80808894b09441d602b4fc779877756d98e976cdd33c64919b13b4f89dfb2d768562be616cbebf375f3f76598ca1dd5a90749e55899222a80ee3c7b15a5
	shasum_data: file - /tmp/data/data4/abc, checksum - 9760a80808894b09441d602b4fc779877756d98e976cdd33c64919b13b4f89dfb2d768562be616cbebf375f3f76598ca1dd5a90749e55899222a80ee3c7b15a5
	shasum_data: file - /tmp/data/data1/abc, checksum - 9760a80808894b09441d602b4fc779877756d98e976cdd33c64919b13b4f89dfb2d768562be616cbebf375f3f76598ca1dd5a90749e55899222a80ee3c7b15a5
	shasum_data: file - /tmp/data/data3/abc, checksum - 9760a80808894b09441d602b4fc779877756d98e976cdd33c64919b13b4f89dfb2d768562be616cbebf375f3f76598ca1dd5a90749e55899222a80ee3c7b15a5
	shasum_data: file - /tmp/data/data3/def, checksum - 59aa91dace300680a842922a75f622bfd9590405ef134a9c6150d7373bf767b2ed9163a0dd9727e7e039d02141b98b6d1114b687cdf29fe0f367b721a0d0531c
	shasum_data: file - /tmp/data/data2/def, checksum - 59aa91dace300680a842922a75f622bfd9590405ef134a9c6150d7373bf767b2ed9163a0dd9727e7e039d02141b98b6d1114b687cdf29fe0f367b721a0d0531c

We can see that 9 files have the md5sum computed, but only 8 files have the 
shasum computed. This is because the checksum 072fb97d7ff234142ed0bea4bce29d0e
(cooresponding to file, /tmp/data/data3/jkl) is unique.

	link_files: linking /tmp/data/data1/def to /tmp/data/data2/def
	link_files: linking /tmp/data/data3/def to /tmp/data/data2/def
	link_files: linking /tmp/data/data4/def to /tmp/data/data2/def
	link_files: linking /tmp/data/data4/ghi to /tmp/data/data1/ghi
	link_files: linking /tmp/data/data3/abc to /tmp/data/data1/abc
	link_files: linking /tmp/data/data4/abc to /tmp/data/data1/abc
	link_files: linking /tmp/data/data2/abc to /tmp/data/data1/abc

We only have three distinct files which are duplicated, so the final phase
consolidates the data; the duplicates are deleted and the links created.

	$ find /tmp/data -type f -exec ls -l "{}" \; | sort
	-rw-r--r-- 1 bob bob 4 Jan 20 18:46 /tmp/data/data1/xyz
	-rw-r--r-- 1 bob bob 7 Jan 20 18:46 /tmp/data/data3/jkl
	-rw-r--r-- 2 bob bob 7 Jan 20 18:46 /tmp/data/data1/ghi
	-rw-r--r-- 2 bob bob 7 Jan 20 18:46 /tmp/data/data4/ghi
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data1/abc
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data1/def
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data2/abc
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data2/def
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data3/abc
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data3/def
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data4/abc
	-rw-r--r-- 4 bob bob 7 Jan 20 18:46 /tmp/data/data4/def
	$ 
	
We can compare this to the original file listing and see that the files
have been linked appropriately.

I typically run it overnight on a hard disk, and usually see that
I can reclaim 5 or 10% of the used disk space.
